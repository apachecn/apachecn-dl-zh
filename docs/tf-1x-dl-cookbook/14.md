# 十四、TensorFlow 处理单元

Google 服务（例如 Google 搜索（RankBrain），街景，Google 照片和 Google 翻译）有一个共同点：它们都使用 Google 的 Tensor 处理单元或 **TPU** 进行计算。

您可能在想什么是 TPU，这些服务有什么好处？ 所有这些服务都在后台使用最新的机器学习算法，并且这些算法涉及大量计算。 TPU 有助于加速所涉及的神经网络计算。 甚至 AlphaGo，一种在 Go 游戏中击败 Lee Sedol 的深度学习程序，都由 TPU 推动。 因此，让我们看看 TPU 到底是什么。

TPU 是 Google 专门为机器学习而定制的定制专用集成电路（**ASIC**），是针对 Tensorflow 量身定制的。 它基于 28 纳米工艺构建，运行频率为 700 MHz，运行时消耗 40 W 的能量。 它包装为外部加速卡，可以插入现有的 SATA 硬盘插槽中。 TPU 通过 PCIe Gen 3×16 总线连接到主机 CPU，该总线提供 12.5 GB/s 的有效带宽。

到目前为止，第一代 TPU 的目标是推理，即使用已经训练好的模型。 DNN 的训练通常需要更多时间，但仍在 CPU 和 GPU 上进行。 在 [2017 年 5 月的博客文章](https://www.blog.google/topics/google-cloud/google-cloud-offer-tpus-machine-learning/)中宣布的第二代 TPU 都可以训练和推断机器学习模型。

# TPU 的组件

在本书涵盖的所有深度学习模型中，无论学习范例如何，都需要进行三个基本计算：乘法，加法和激活函数的应用。

前两个成分是矩阵乘法的一部分：权重矩阵`W`需要与输入矩阵`X`相乘`W^T · X`； 矩阵乘法在 CPU 上的计算量很大，尽管 GPU 使操作并行化，但仍有改进的余地。

TPU 具有 65,536 个 8 位整数矩阵乘法器单元（**MXU**），峰值吞吐量为 92 TOPS。 GPU 和 TPU 乘法之间的主要区别在于 GPU 包含浮点乘法器，而 TPU 包含 8 位整数乘法器。 TPU 还包含一个统一缓冲区（**UB**），用作寄存器的 24 MB SRAM 和一个包含硬接线激活函数的激活单元（**AU**）。

MXU 是使用脉动阵列架构实现的。 它包含一个阵列算术逻辑单元（ALU），该阵列连接到网状拓扑中的少量最近邻居。 每个数据值仅读取一次，但在流过 ALU 数组时会多次用于不同的操作，而无需将其存储回寄存器。 TPU 中的 ALU 仅以固定模式执行乘法和加法。 MXU 已针对矩阵乘法进行了优化，不适用于通用计算。

每个 TPU 还具有一个片外 8GiB DRAM 池，称为加权存储器。 它具有四个阶段的流水线，并执行 CISC 指令。 到目前为止，TPU 由六个神经网络组成：两个 MLP，两个 CNN 和两个 LSTM。

在高级指令的帮助下对 TPU 进行编程； 下面是一些用于对 TPU 进行编程的指令：

*   `Read_Weights`：从内存读取权重
*   `Read_Host_Memory`：从内存中读取数据
*   `MatrixMultiply/Convolve`：与数据相乘或卷积并累加结果
*   `Activate`：应用激活函数
*   `Write_Host_Memory`：将结果写入存储器

Google 创建了一个 API 堆栈，以方便 TPU 编程； 它将来自 Tensorflow 图的 API 调用转换为 TPU 指令。

# TPU 的优势

TPU 提供的优于 GPU 和 CPU 的首要优势是性能。 Google 将 TPU 的性能与运行基准代码（代表 95% 的推理工作量）的服务器级 Intel Haswell CPU 和 NVIDIA K80 GPU 进行了比较。 它发现 TPU 的速度比 NVIDIA GPU 和 Intel CPU 快 15-30 倍。

第二个重要参数是功耗。 降低功耗非常重要，因为它具有双重能源优势：它不仅减少了功耗，而且还通过降低散热成本来散热，从而节省了功耗，从而消除了加工过程中产生的热量。 TPU / CPU 每瓦性能比其他 CPU 和 GPU 配置提高了 30-80 倍。

TPU 的另一个优点是其最小化和确定性的设计，因为它们一次只能执行一个任务。

与 CPU 和 GPU 相比，单线程 TPU 没有任何复杂的微体系结构功能会消耗晶体管和能量来改善平均情况，但不会消耗 99% 的情况：没有缓存，分支预测，乱序执行， 多处理，推测性预取，地址合并，多线程，上下文切换等。 极简主义是特定领域处理器的优点。

# 访问 TPU

Google 已决定不直接将 TPU 出售给他人； 取而代之的是，将通过 Google 云平台提供 TPU：[Cloud TPU Alpha](https://cloud.google.com/tpu/)。 Cloud TPU Alpha 将提供高达 180 teraflops 的计算性能和 64 GB 的超高带宽内存。 用户将能够从自定义虚拟机连接到这些 Cloud TPU。

Google 还决定向全球的机器学习研究人员免费提供 1000 个云 TPU 集群，以加快开放式机器学习研究的步伐。 在有限的计算时间内，将授予选定的个人访问权限； [个人可以使用以下链接进行注册](https://services.google.com/fb/forms/tpusignup/)。 根据 Google Blog：

“由于 TensorFlow 研究云的主要目标是使整个开放式机器学习研究社区受益，因此，成功的申请人有望做到以下几点：

通过同行评审的出版物，开源代码，博客文章或其他开放媒体与世界分享其 TFRC 支持的研究

与 Google 分享具体的建设性反馈，以帮助我们随着时间的推移改进 TFRC 计划和基础的 Cloud TPU 平台。

想象一下 ML 加速丰富的未来，并基于这种未来开发新的机器学习模型。”

# TPU 上的资源

*   Norman P.Jouppi 等人，张量处理单元的数据中心内性能分析，arXiv：1704.04760（2017）。 在本文中，作者将 TPU 与服务器级的 Intel Haswell CPU 和 NVIDIA k80 GPU 进行了比较。 本文以 TPU 与 CPU 和 K80 GPU 的性能为基准。
*   [此 Google 博客通过以下简单术语说明了 TPU 及其工作原理](https://cloud.google.com/blog/big-data/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu)